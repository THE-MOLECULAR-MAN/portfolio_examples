{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a Machine Learning model for detecting Internet of Things Malware\n",
    "\n",
    "TODO:\n",
    "* Change export format from CSV to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# %pip install --upgrade --quiet pip geoip2 humanize pyarrow\n",
    "# print('Finished installing dependencies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built in modules\n",
    "from datetime import datetime\n",
    "time_start_notebook =  datetime.now()\n",
    "import os\n",
    "import importlib\n",
    "import ipaddress\n",
    "import glob\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# PIP modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import geoip2.database\n",
    "import humanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DON'T TOUCH ANYTHING BELOW #########################\n",
    "newpath = '..'\n",
    "if not newpath in sys.path:\n",
    "    sys.path.insert(1, newpath)\n",
    "import tim_ml_lib as tml    # works\n",
    "importlib.reload(tml)     # reload it since I'm frequently making changes.\n",
    "######################### DON'T TOUCH ANYTHING ABOVE #########################\n",
    "RANDOM_STATE        = tml.settings.RANDOM_STATE\n",
    "\n",
    "# some common, basic setup\n",
    "tml.initialize_random_seeds()\n",
    "tml.initialize_display_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILEPATH   = './data/CTU-IoT-Malware-Capture_ready2train.csv.gz'\n",
    "CSV_FILES_TO_LOAD = glob.glob('./data/CTU-IoT-Malware-Capture*.labeled.csv.gz')\n",
    "\n",
    "ORIGINAL_LABEL_COLUMN_NAME = 'label'\n",
    "LABEL_COLUMN_NAME          = 'is_IoT_malware'\n",
    "\n",
    "# Using Python's built-in datatypes\n",
    "# highest compatibility, poorest speed performance\n",
    "# SMALL_INT_DATATYPE  = 'Int8'\n",
    "# BIG_INT_DATATYPE    = 'Int64'     # 'uint64[pyarrow]'\n",
    "# PORT_DATATYPE       = 'Int32'     # 'uint32[pyarrow]'\n",
    "# FLOAT_DATATYPE      = 'float32'   # 'float32[pyarrow]'\n",
    "# ENUM_DATATYPE       = 'category'\n",
    "# BOOL_DATATYPE       = 'bool'      # 'bool[pyarrow]'\n",
    "\n",
    "# Using pyarrow datatypes\n",
    "# SMALL_INT_DATATYPE  = 'uint8[pyarrow]'\n",
    "# BIG_INT_DATATYPE    = 'uint64[pyarrow]'\n",
    "# PORT_DATATYPE       = 'uint32[pyarrow]'\n",
    "# FLOAT_DATATYPE      = 'float32[pyarrow]'\n",
    "# ENUM_DATATYPE       = 'category'\n",
    "# BOOL_DATATYPE       = 'bool[pyarrow]'\n",
    "\n",
    "# Using Numpy datatypes\n",
    "# compatible with more functions, like correlation\n",
    "SMALL_INT_DATATYPE  = np.uint8\n",
    "BIG_INT_DATATYPE    = np.uint64\n",
    "PORT_DATATYPE       = np.uint32\n",
    "FLOAT_DATATYPE      = np.float32\n",
    "ENUM_DATATYPE       = 'category'\n",
    "BOOL_DATATYPE       = bool\n",
    "\n",
    "# https://stackoverflow.com/questions/29245848/what-are-all-the-dtypes-that-pandas-recognizes\n",
    "IMPORT_DATATYPES = {  \n",
    "    # 'uid':          'string',           # unique identifier\n",
    "    'ts':           FLOAT_DATATYPE,   # timestamp, not really needed unless I want to extract the hour\n",
    "    #'id.resp_p':    PORT_DATATYPE,         # The destination port\n",
    "    'id.orig_p':    PORT_DATATYPE,         # The source      port\n",
    "    # 'id.orig_h':    'string',         # The source      IP address\n",
    "    # 'id.resp_h':    'string',         # The destination IP address\n",
    "    # 'local_orig':   BOOL_DATATYPE',    # ALL ARE NULL - Indicates whether the connection is considered local or not - Bool\n",
    "    # 'local_resp':   BOOL_DATATYPE,    # ALL ARE NULL - Indicates whether the connection is considered local or not - Bool\n",
    "    # 'history':      'string',           # A history of connection states - String\n",
    "    'label':          ENUM_DATATYPE,         # A label associated with the connection (e.g., 'Malicious' or 'Benign'). string\n",
    "    # 'conn_state':     ENUM_DATATYPE,       # The state of the connection.|string|\n",
    "    'proto':          ENUM_DATATYPE,         # The network protocol used (e.g., 'tcp').\n",
    "    'service':        ENUM_DATATYPE,         # The service associated with the connection.|string\n",
    "    # 'tunnel_parents': ENUM_DATATYPE,        # ALL ARE NULL - Indicates if this connection is part of a tunnel.|set[string]|\n",
    "    # 'detailed-label': ENUM_DATATYPE,       # A more detailed description or label for the connection.|string|\n",
    "    'duration':       FLOAT_DATATYPE,     # has NaN values, so have to use float; The duration of the connection. DECIMAL number in seconds?\n",
    "    'orig_bytes':     FLOAT_DATATYPE,     # has NaN values, so have to use float; The number of bytes sent from the source to the destination.\n",
    "    'resp_bytes':     FLOAT_DATATYPE,     # has NaN values, so have to use float; The number of bytes sent from the destination to the source\n",
    "    'missed_bytes':   BIG_INT_DATATYPE,       # The number of missed bytes in the connection.\n",
    "    'resp_ip_bytes':  FLOAT_DATATYPE,     # The number of IP bytes sent from the destination to the source.\n",
    "    'resp_pkts':      FLOAT_DATATYPE,     # The number of packets sent from the destination to the source\n",
    "    'orig_pkts':      BIG_INT_DATATYPE,       # cannot use 32-bit types; The number of packets sent from the source to the destination.\n",
    "    'orig_ip_bytes':  BIG_INT_DATATYPE, # cannot use 32-bit types; The number of IP bytes sent from the source to the destination.|count|\n",
    "}\n",
    "\n",
    "raw_cols_to_import = IMPORT_DATATYPES.keys()\n",
    "\n",
    "# COLUMNS_TO_OHE = ['proto', 'service', 'conn_state', \n",
    "#                   'history', 'ip_dest_country'] \n",
    "#                     #'id.resp_h', 'id.orig_h']\n",
    "                    \n",
    "# SERVICE_TO_PROTOCOL_AND_PORT_MAPPINGS = {\n",
    "#   'ssh': {'protocol': 'tcp', 'port': 22},\n",
    "#   'dns': {'protocol': 'udp', 'port': 53},\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data into a Pandas dataframe\n",
    "load a directory of CSV files\n",
    "\n",
    "currently takes 82 sec on M3 laptop\n",
    "\n",
    "cut down to 36 sec when I only imported the columns that I needed\n",
    "\n",
    "cut from 36 to 26 by specifying uint[pyarrow] instead of Int64, not changing engine\n",
    "\n",
    "* https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-chunking\n",
    "* https://stackoverflow.com/questions/66346343/can-i-load-multiple-csv-files-using-pyarrow\n",
    "* https://arrow.apache.org/docs/2.0/python/generated/pyarrow.csv.ReadOptions.html#pyarrow.csv.ReadOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []    # create an empty list to contain all the dataframes\n",
    "for iter_csv_file in CSV_FILES_TO_LOAD:\n",
    "\n",
    "    # print(iter_csv_file)\n",
    "    df_temp = pd.read_csv(iter_csv_file,\n",
    "                          # index_col='uid',\n",
    "                          # index_col=False,\n",
    "                          # chunksize=?,\n",
    "                          # engine='pyarrow',        # causes issues with delimiters sometimes\n",
    "                          engine='c',        \n",
    "                          # converters={}, # to investigate\n",
    "                          dtype=IMPORT_DATATYPES,\n",
    "                          usecols=raw_cols_to_import,            # only load certain columns\n",
    "                          delimiter='|',\n",
    "                          na_values='-'\n",
    "    )\n",
    "    dfs.append(df_temp)\n",
    "    del df_temp\n",
    "\n",
    "df_orig = pd.concat(dfs, ignore_index=True)\n",
    "df_orig = df_orig.astype(IMPORT_DATATYPES)        # force it again, necessary\n",
    "df_orig = tml.drop_totally_empty_columns(df_orig)\n",
    "del dfs\n",
    "\n",
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial EDA on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tml.list_percent_of_rows_missing_values(df_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tml.get_total_dataframe_memory_usage(df_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tml.get_dataframe_memory_usage_by_col(df_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benign                                   .3510518     # 35% of whole entire dataset is benign\n",
    "# tml.get_class_balance(df_orig, label_col_name=ORIGINAL_LABEL_COLUMN_NAME).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a copy for easy retesting\n",
    "Easy restart point during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig.copy()\n",
    "\n",
    "importlib.reload(tml)     # reload it since I'm frequently making changes.\n",
    "tml.initialize_random_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep - locating and replacing missing values\n",
    "\n",
    "```\n",
    "service       0.999280\n",
    "duration      0.610614\n",
    "orig_bytes    0.610614\n",
    "resp_bytes    0.610614\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View missing values before replacement\n",
    "# tml.list_percent_of_rows_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na_with = {\n",
    "    'duration':       0,\n",
    "    'orig_bytes':     0,\n",
    "    'resp_bytes':     0,\n",
    "    'missed_bytes':   0,\n",
    "    'resp_ip_bytes':  0,\n",
    "    'resp_pkts':      0,\n",
    "    'orig_pkts':      0,\n",
    "    'orig_ip_bytes':  0,\n",
    "}\n",
    "\n",
    "df.fillna(fill_na_with, inplace=True)\n",
    "\n",
    "# View missing values after replacement\n",
    "tml.list_percent_of_rows_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the label and determining class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the label as boolean\n",
    "df[LABEL_COLUMN_NAME] = ~df[ORIGINAL_LABEL_COLUMN_NAME].isin(['Benign'])\n",
    "df.drop(columns=[ORIGINAL_LABEL_COLUMN_NAME], inplace=True)\n",
    "tml.get_class_balance(df, label_col_name=LABEL_COLUMN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature addition - account for the time of day that attackers tend to attack \n",
    "Increases the correlation for the time and label\n",
    "\n",
    "1) Extracts the hour from the timestamp [0,23]\n",
    "2) Applies an offset to account for when attackers work [0,23]\n",
    "3) Normalizes the day to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_hour_of_day(orig_hour_value, shift_amount=0):\n",
    "    \"\"\" Offsets an hour of the day. Used for moving around time for\n",
    "    adjusting for work days, trying to keep it linear so that events that tend to happen after hours are more linearly related.\n",
    "    Returns a number [0,23]\"\"\"\n",
    "    return abs((orig_hour_value + shift_amount) % 24)\n",
    "\n",
    "\n",
    "def adjust_for_attacker_workday(df1):\n",
    "    \"\"\"x\"\"\"\n",
    "    df = df1.copy()\n",
    "    df['hour_of_day'] = df['hour_of_day'].map(lambda x: shift_hour_of_day(x, shift_amount=-18))\n",
    "    return df\n",
    "\n",
    "\n",
    "# converting the date to timestamp, need the unit='s' to convert Unix time\n",
    "df['ts'] = pd.to_datetime(df['ts'], errors=\"raise\", unit='s')\n",
    "\n",
    "# Add feature - hour of day\n",
    "df['hour_of_day'] = df['ts'].dt.hour + (df['ts'].dt.minute / 60.0)\n",
    "df.drop(columns=['ts'], inplace=True)   # drop the original\n",
    "\n",
    "df = adjust_for_attacker_workday(df)\n",
    "df = tml.normalize_to_specific_range(df, 'hour_of_day', 0, 23)\n",
    "\n",
    "# p0 = sns.histplot(data=df, x='hour_of_day', hue=LABEL_COLUMN_NAME, binwidth=0.25)\n",
    "# plt.ylim(0,250000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature addition: mapping port numbers to ranges\n",
    "While the port number itself is important, and can give clues about the traffic, which port range it falls into also give high level information.\n",
    "\n",
    "* Well-known ports: 0–1023, also known as system ports, are used by common network services and official ports for notable system administrations\n",
    "* Registered ports: 1024–49151, also known as user ports, are used for a variety of network services and functions\n",
    "* Dynamic and private ports: 49152–65535, also known as ephemeral ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 16 sec before vectorization\n",
    "def map_port_number_to_range(port_number):\n",
    "    \"\"\"x\"\"\"\n",
    "    \n",
    "    #if not pd.api.types.is_numeric_dtype(port_number):\n",
    "    #    return ''\n",
    "    # if not 1 <= port_number <= 65535:\n",
    "    #     return ''\n",
    "    \n",
    "    if 1 <= port_number <= 1023:\n",
    "        return 0 #'0-reserved'\n",
    "    elif 1024 <= port_number <= 49151:\n",
    "        return 1  # '1-registered'\n",
    "    elif 49151 <= port_number <= 65535:\n",
    "        return 2  # '2-ephemeral'\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "df['src_port_range_name'] = df['id.orig_p'].apply(map_port_number_to_range)\n",
    "df = df.astype({'src_port_range_name': SMALL_INT_DATATYPE})\n",
    "df.drop(columns=['id.orig_p'], inplace=True)\n",
    "\n",
    "df['src_port_range_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.set_index('uid', inplace=True)     # causes issues sometimes\n",
    "# IP_ADDRESS_COLUMN_NAMES = ['id.orig_h', 'id.resp_h']\n",
    "# for iter_colname in IP_ADDRESS_COLUMN_NAMES:\n",
    "#     df[iter_colname] = df[iter_colname].apply(ipaddress.ip_address)\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some sample data after the transforms\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature addition / correction - account for the time of day that attackers tend to attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature addition: ratio of incoming vs outgoing bandwidth, packets, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['orig_bytes_clipped']    = df_orig['orig_bytes'].clip(upper=50000)\n",
    "# df['resp_bytes_clipped']    = df_orig['resp_bytes'].clip(upper=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.scatterplot(data=df, x='orig_bytes_clipped', y='resp_bytes_clipped', hue=LABEL_COLUMN_NAME)\n",
    "# plt.loglog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Correlation with label was \"NaN\"\n",
    "# df['test1']                 =  df_orig['resp_bytes'].apply(np.log) / df_orig['orig_bytes'].apply(np.log)\n",
    "# df['test1_std'] = stats.zscore(df['test1'])\n",
    "# sns.histplot(data=df, x='test1', hue=LABEL_COLUMN_NAME)\n",
    "# plt.ylim((0,2500))\n",
    "# plt.xlim((0.5,2.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Correlation with label was \"NaN\"\n",
    "# df['test2']                 =  (df_orig['resp_bytes'] / df_orig['orig_bytes']).apply(np.log)\n",
    "# sns.histplot(data=df, x='test2', hue=LABEL_COLUMN_NAME)\n",
    "# plt.ylim((0,2000))\n",
    "# plt.xlim((-2.5,4.5))\n",
    "# df['test2_std'] = stats.zscore(df['test2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test 0 - no log stuff\n",
    "# df['ratio_bytes']   = df_orig['resp_bytes'] / df_orig['orig_bytes']\n",
    "# df['ratio_bytes'].describe()\n",
    "# sns.histplot(data=df, x='ratio_bytes', hue=LABEL_COLUMN_NAME, log_scale=(False, True), binwidth=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ratio_packets'] = df['orig_pkts']  / df['resp_pkts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature addition: Geo-IP and ASN features\n",
    "This step takes the longest - about 8 minutes on a MacBook air w/o vectorizing via numba or specifying # of threads via "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # configure and load the GeoIP databases\n",
    "# # https://dev.maxmind.com/geoip/geolite2-free-geolocation-data?lang=en  \n",
    "# # https://www.maxmind.com/en/accounts/985797/geoip/downloads\n",
    "# # https://github.com/maxmind/GeoIP2-python?tab=readme-ov-file#database-usage\n",
    "\n",
    "# try:\n",
    "#     # open the readers, they must be closed below\n",
    "#     geoip_country = geoip2.database.Reader('./geoip/GeoLite2-Country_20240308/GeoLite2-Country.mmdb')\n",
    "#     geoip_asn     = geoip2.database.Reader('./geoip/GeoLite2-ASN_20240308/GeoLite2-ASN.mmdb')\n",
    "\n",
    "#     # @numba.vectorize\n",
    "#     def ip_to_country(ip_as_str):\n",
    "#         try:\n",
    "#             ip = ipaddress.ip_address(ip_as_str)\n",
    "#             if ip.is_global:\n",
    "#                 return geoip_country.country(ip).country.name\n",
    "#         finally:\n",
    "#             return None\n",
    "\n",
    "#     # @numba.vectorize\n",
    "#     def ip_to_asn(ip_as_str):\n",
    "#         try:\n",
    "#             ip = ipaddress.ip_address(ip_as_str)\n",
    "#             if ip.is_global:\n",
    "#                 return geoip_asn.asn(ip).autonomous_system_number\n",
    "#         finally:\n",
    "#             return None\n",
    "\n",
    "\n",
    "#     # GeoIP\n",
    "#     df['ip_dest_country'] = df['id.resp_h'].apply(ip_to_country)\n",
    "#     df['ip_asn']          = df['id.resp_h'].apply(ip_to_asn)\n",
    "\n",
    "#     print(df['ip_dest_country'].unique().tolist())\n",
    "#     print(df['ip_asn'].unique().tolist())\n",
    "# finally:\n",
    "#     # close the readers\n",
    "#     geoip_country.close()\n",
    "#     geoip_asn.close()\n",
    "\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting unique values\n",
    "# tml.get_unique_value_counts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting strings to one-hot encoded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tml.list_unique_string_values(df)\n",
    "# df['service'].unique()\n",
    "df = tml.onehotencode_single_column(df, 'service')\n",
    "df = tml.onehotencode_single_column(df, 'proto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second phase of EDA - after feature additions \n",
    "but before normalizing and standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_eda(df1, colname, lab, log_scale=True):\n",
    "    \"\"\"x\"\"\"\n",
    "    print(df1[colname].describe(percentiles=[]))\n",
    "    sns.histplot(data=df1, x=colname, hue=lab, log_scale=log_scale)\n",
    "    plt.title(f'Histogram for {colname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick_eda(df, colname='orig_bytes', lab=LABEL_COLUMN_NAME)\n",
    "# plt.xlim(right=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick_eda(df, colname='resp_bytes', lab=LABEL_COLUMN_NAME)\n",
    "# plt.xlim(left=10, right=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick_eda(df, colname='missed_bytes', lab=LABEL_COLUMN_NAME)\n",
    "# # plt.xlim(right=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(data=df, x='resp_pkts', hue=LABEL_COLUMN_NAME, log_scale=(False, True)) #, binwidth=5)\n",
    "# #plt.xlim(left=0, right=400)\n",
    "# #plt.ylim((0,5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick_eda(df, colname='resp_ip_bytes', lab=LABEL_COLUMN_NAME)\n",
    "# # plt.xlim(right=7500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data Prep - Winsorizing and Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_winsor_then_standardize = [\n",
    "    'duration', \n",
    "    'orig_pkts',\n",
    "    'orig_ip_bytes',    \n",
    "    ]\n",
    "\n",
    "for c in cols_to_winsor_then_standardize:\n",
    "    print(f'Winsorizing {c}...')\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.winsorize.html\n",
    "    df[c] = stats.mstats.winsorize(df[c], limits=[0.05, 0.05])\n",
    "    print(f'Standardizing {c}...')\n",
    "    # df = tml.standardize_column_using_zscore(df, c) # causes issues\n",
    "    df[c] = stats.zscore(df[c]) \n",
    "\n",
    "print('Finished prep.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clipping then standardizing\n",
    "\n",
    "df['missed_bytes']  = df['missed_bytes'].clip(upper=25000)\n",
    "\n",
    "df['resp_pkts']     = df['resp_pkts'].clip(upper=100)\n",
    "\n",
    "df['resp_ip_bytes'] = df['resp_ip_bytes'].clip(upper=7500)\n",
    "\n",
    "df['orig_bytes']    = stats.zscore(df['orig_bytes'])\n",
    "df['resp_bytes']    = stats.zscore(df['resp_bytes'])\n",
    "df['missed_bytes']  = stats.zscore(df['missed_bytes'])\n",
    "df['resp_pkts']     = stats.zscore(df['resp_pkts'])\n",
    "df['resp_ip_bytes'] = stats.zscore(df['resp_ip_bytes'])\n",
    "\n",
    "# TODO: feature idea: ratio of packets or size to/from source/dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tml.trainable_data_report(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data Prep - Converting Bools to Ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tml.change_all_bools_to_ints(df)   # must be done befoe correlations\n",
    "tml.feature_target_correlation(df, label_column_name=LABEL_COLUMN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tml.get_total_dataframe_memory_usage(df_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final tests before export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id.orig_p            102304\n",
    "# id.resp_p             75872\n",
    "# tml.get_unique_value_counts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tml.assert_datatypes_ready_for_training(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tml.get_total_dataframe_memory_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tml)\n",
    "tml.feature_target_correlation(df, label_column_name='is_IoT_malware')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features with poor label correlation:\n",
    "importlib.reload(tml)\n",
    "df = tml.drop_feature_below_corr_threshold(df, 0.04, label_column_name='is_IoT_malware')\n",
    "tml.feature_target_correlation(df, label_column_name='is_IoT_malware')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting ready to train data to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime on Macbook air with full dataset to uncompressed file: at least 8 min\n",
    "# M3 runtime to GZ with several other changes like datatypes, but no multicore stuff: 1 min 22 sec, 37.6 MB\n",
    "# M3 runtime to CSV with several other changes like datatypes, but no multicore stuff: 50 sec, 1.4 GB\n",
    "#   CSV is mariginally faster than XZ, but takes up way more space\n",
    "#   https://dask.pydata.org/en/latest/diagnostics-local.html\n",
    "#   increase # of rows/block size\n",
    "#   https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html\n",
    "\n",
    "# shuffle the order randomly before outputting.\n",
    "df = df.sample(frac=1,random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# Create a training/test dataset and output to CSV\n",
    "df.to_csv(OUTPUT_FILEPATH,\n",
    "        sep=',',                        # changing delimiter to , so that pyarrow engine can be used by model\n",
    "        header=True,\n",
    "        index=False,\n",
    "        chunksize=1000000,\n",
    "        compression='gzip',\n",
    "        quoting=csv.QUOTE_ALL,\n",
    "        encoding='utf-8')\n",
    "\n",
    "print(f\"Training data saved to new file:\\n{OUTPUT_FILEPATH}\")\n",
    "\n",
    "output_filesize = humanize.naturalsize(os.stat(OUTPUT_FILEPATH).st_size)\n",
    "print(f'output file size: {output_filesize}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end_notebook =  datetime.now()\n",
    "print(time_end_notebook - time_start_notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
